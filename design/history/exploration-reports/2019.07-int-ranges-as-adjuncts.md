What to do about integer range?
===============================

Concepts
--------

### cardinality

See the [Cardinality](../../../concepts/type-theory-glossary.md#cardinality) chapter
in the Type Theory Glossary concepts doc.

### totality

A "total function" is one that has defined outputs for all inputs.

A "partial function" is one that has undefined outputs for some inputs.

### bijection

[Wikipedia has a particularly nice diagram](https://en.wikipedia.org/wiki/Injective_function).

### overflow

Integer overflow can be defined as when one fails to have a bijection.

Overflow without explicit error is a surjection (it silently returns another number).

Overflow with explicit error is a partial function.

(Overflow with explicit error can also be visualized as having a codomain
which is tuples of numbers and errors, but it's unclear if this produces
any more useful intuitions than simply considering it a partial function.)


Concerns
--------

### overflow is annoying

This is basic, but let's say it.

Overflow is a source of bugs (especially when it occurs at runtime and
without explicit error).

Overflow is also a potential source of *design* bugs: if two communicating
systems have different numeric support ranges, that almost certainly indicates
a potential compatibility issue.
(Whether or not such a compatibility issue is reachable in practical usage may
be another matter, but the possibility itself is identifiable.)

### we need to support "large enough" numbers

We might not be able to remove the concept of overflow from the world.

Knowing that overflow is a source of problems, however, we should at least
be able to specify when and where it's going to affect us.

We should be able to be clear about any limits to the precision of numbers
we can handle.  If an application needs to serialize and deserialize numbers
between 1 and 10 billion, it might be important to declare this: such an
application will not work with libraries limited to 32 bit integer precision!

### let's not have dependent types

Dependent types, as often commented, are cool;
but are the kind of powerful that comes with costs.

Specifically, dependent types make it easy for users to create *new cardinalities*,
and this tends to drastically increase the number of places where we might encounter
non-total functions.  Reducing the number of places that can happen -- or at
least, making sure if it happens, it's consistently schema-side or consistently
user-application-side -- is desirable.

### built in ranges are softcore dependent types

"uint8" === "int [0 < 255]".

There's no particular reason to bless "uint8" any more than there is "int [0 < 12]"...
other than mechanical sympathy with machine architectures that have physically
special understanding of uint8.

Mechanical sympathy is good -- but it's worth pointing out that when that's the aim.
Ideally, we want to get mechanical sympathy without too many compromises, or
with well-boxed compromises.

### IPLD has no math functions

IPLD has no math functions!  This actually means we get to play on "easy mode".

It's worth pointing this out because it means we have a lot fewer opportunities
for overflows to come up than a programming language would.
All we have to worry about is round-tripping the numbers through any of our
mappings like serialization, and making numbers available in our client library
interfaces without loss of precision (or at least with clear loss of precision).

This is nice because it's a lot easier to reason about simple mappings from
one domain to another than it is to worry about e.g. multiplication or addition
taking two individually fine numbers and producing an overflowing one.
If there is any math, it's in an application that's using one of our libraries,
and the precision and overflow handling story for the math is entirely the
responsibility of the system doing the math.

### practical applications

#### timestamps are a real issue

One very frequent occurrence of practical concerns about numeric ranges is in
timestamps.

- 2^31 as a unix timestamp int is Tue, 19 Jan 2038 -- fast approaching!
- 2^32 as a unix timestamp int is Sun, 07 Feb 2106 -- also not far!
- 2^63 as a unix timestamp int is just shy of 300 billion years out.

In short: 32-bit ints (either signed or unsigned) are problematic for
timestamps in our lifetime; 64-bit (signed or unsigned) are sufficient.

#### nonces

... are mostly *not* an issue.  If doing random numbers for security purposes,
applications should use Bytes, and include an application-level check that any
nonce is of reasonable length to meet the application's security needs.

(Min-length arrays are again wandering into dependent type land.)

#### banking

Large numbers in financial systems care about precision.
(However, it should also be noted that This Is Not New to that field,
and financial systems already often use their own bignum systems,
which we will likely not have an effect on.  Such systems will most likely
simply appear as a "bytes" kind in IPLD Schemas.)

#### engineering and scientific compute

See 'banking'.  In general, it's not uncommon for systems dealing with very
large numbers to have an existing bignum concept of some kind.


The Idea
--------

Let's use _adjuncts_.

This won't be a complete solution to the general problem,
but it will definitely give us some wiggle room to solve it in some cases,
and especially solve it in the codegen case,
and it'll do it without complicating either the Data Model nor the Schemas!

### what's an adjunct

Adjunct information is something attached to a schema which contains additional
strong suggestions about what to do when implementing it.  Adjunct information
is less formally specified than the rest of the schema system, and importantly,
adjunct information is allowed to contain language-specific notes as well
as other specializations.

The only rule for adjunct information is that it must not change the cardinality
of types nor the representation, because those are both kinds of information
that belong in the schema.

So, it would seem that we could say that some adjunct information may provide
hints which change the cardinality which is *supported* by a client library...
even though we'll still theoretically consider the cardinality unchanged for
any schematic purpose.

(There aren't many proposed uses of adjunct information yet, but it's
anticipated that the subject will start coming up, especially as we proceed with
codegen.  So, while it may sound unfamiliar now, it may not be in the future!)

### what would this look like

Perhaps with a `foobar.ipldsch` file like the following:

```ipldsch
type Foo struct {
	field SnazzyInt # note no size specifier
}

type SnazzyInt int # note no size specifier
```

... we'd then have a sibling `foobar.adjunct.go.cfg` file:

```
codegen.forType.SnazzyInt.nativeSize = "uint64"
```

(Syntax up for grabs.  This is a very vague, early proposal.
We could mine other systems like protobufs for their syntax as well;
I believe there are comparable features, though I don't know details.)

(Another alternative would be to specify things by the fieldname in a struct,
thus working even without the need for a named type for making size specifiers.
This might be interesting to explore further, but seems fraught with peril.)

### what does this give us

As the example above shows, this would give us a way to specify that a client
library needs to support a certain size of numeric precision (in this case,
"uint64").

We could also specify that a code-generating client should make types in its
target language which use smaller memory layouts, like uint8: in some
applications, this may be relevant to performance.

### why is the better than alternatives

Adding a proliferation of {u,}int{,8,16,32,64} types to the Data Model causes
the number of kinds to more than double.  The complexity cost is significant.
Using adjuncts to define numeric range support avoids this complexity.

### what doesn't this solve

Long story short, specifying demands for large numeric ranges solves several
problems for generated code, but does *not* solve problems for general-purpose
"Node" interface design in strongly-typed languages.

(Of course, we can't wave a magic wand to give math in client languages
infinite precision either -- but that's nothing new!)

#### still several int getters

In building "Node" interfaces in most strongly typed languages, we'll still
often need 3 methods for reading ints, which is about 2 more than desired:

- `AsInt` -> returns `int` (the convenient, most-likely-to-be-used type in the language)
- `AsInt64` -> or whatever the largest signed int type is in the language
- `AsUInt64` -> or whatever the largest unsigned int type is in the language

The first method is required for convenience (most language won't allow their
`int64` type to be silently downcast to `int`); the latter two for correctness
in handling larger integers.

(I've made the assumption here that if using unsigned ints, it's because you
cared about every last bit, and therefore uint methods for anything other than
the largest size are unnecessary.)

(Note that we can probably get away with only two *setter* methods, since
languages will typically upcast `int` to `int64` silently.)

#### the convenient int getter will have to check overflow

The `AsInt` method, which returns the most-likely-to-be-used type in the
language (for sheer syntactic convenience), will need to check for overflow
in many cases.

Code-generated node implementations may be free of this, since they already
know their limits, but other generic storage node implementations will be
stuck checking for overflow.

#### generic node implementations aren't improved

Generic node implementations (i.e. can contain any value at runtime; none of
the NodeBuilder methods will reject anything) will have to use `int64`, or
whatever the largest form of int is, in their internal storage.

This might be fine -- but it's worth noting.

Note we also didn't solve the uint64 problem yet.

#### generic user code always needs to use the biggest form

Generic code will inevitably need to use the biggest numeric getters in order
to be correct.  (There's no way to reference adjunct configuration from a schema
from code that's targeting the general "Node" interface.)

This might be fine -- but it's worth noting.

Note we also didn't solve the uint64 problem yet.

#### unsigned ints are frustrating

Unsigned ints are effectively one bit bigger than signed ints.

If we try to have a single int type in the data model, but support uints
for reasons of that extra bit of range, things get painful.
Imagine writing a "deep copy" function over the generic Node interface:
you reach a node of kind int: what do you do?  Use GetInt() or GetUint()?


Nearby alternatives
-------------------

### ditch the convenient int method

Maybe we don't really need it.

Will involve a lot of user code writing their own casts if they want to use
the "Node" interface and want to use `int` rather than `int64` throughout
their application code, though.

### do we really need uint?

Do we?  It seems like a fairly massive amount of trouble.

There are no obvious solutions to the
[unsigned-ints-are-frustrating](#unsigned-ints-are-frustrating) issue,
except for introducing at least one new kind to the Data Model;
and that kind won't be representable or meaningful in a fair number of
Codecs (neither JSON *nor CBOR* retains uint info), which would
seem to strongly indicate it *does not belong* in our Data Model.


Worth mentioning
----------------

The following facts are interesting, but only tangentially relevant:

### CBOR does not have signed and unsigned

CBOR has *negative ints* and *positive ints*.

Though the documentation refers to this as "unsigned", this is... contendable.
It is not possible to express an "unsigned positive" distinctly from "signed positive";
therefore, it seems inaccurate to say CBOR supports unsigned and signed ints.

### CBOR big ints

BigInt *is* defined in the CBOR spec -- however, as an extension, based on tags.
See: https://tools.ietf.org/html/rfc7049#section-2.4.2 .

Support for this in the wild is not common.  It's not clear if we'd have much
to gain from trying to transparently use this rare feature of a single codec.

### CBOR fraction types

Fractions are also defined in the CBOR spec -- again, as an extension with tags.
See: https://tools.ietf.org/html/rfc7049#section-2.4.3 .

Support for this in the wild is not common.  Furthermore, it's not clear how
we could even begin to consider natively support this in IPLD without adding
"Fraction" as a kind to the Data Model.

The implementation details of this fraction system might be of interest to
anyone who wants to implement fractions in their application, though
(we could imagine using a simple struct in a schema to represent fractions!).


Lessons learned
---------------

Overflows are still everywhere.

The best we can do is make a table about where we want to shuffle them off to.

That table has to include every ingoing and outgoing edge in client libraries,
which makes it pretty big.  As a heuristic for success: the smaller we can
keep this table, while still providing our utility, the better.

And the table might have separate consideration of codegen'd native-type node
implementations vs generic node implementations.  (Codegen'd/native-type nodes
can be interfaced with in more ways than generic nodes, which relieves some
pressure; the question then becomes if that's actually grounds to have weaker
support in the generic nodes (to which I suspect the answer is mostly "no").)

The adjunct idea may (or may not) be a large part of a good solution;
the one thing that's clear is it's not a complete solution on its own,
because we still need specs for generic Node behavior where this isn't applicable.

Trying to imagine writing a "deep copy" function that works over the generic
Node interface is a good forcing function to think through all the implications
of a design.  (Nevermind that a "deep copy" function is nonsensical in a library
that has an immutable/copy-on-write design: imagine doing it anyway, perhaps for
example to flip data from a codegen-backed Node implemention to a generic one,
and back again.)  (Could we specify a system that is lossless when used with a
codegen-backed Node and a particular codec, but lossy when copied through the
generic Node interfaces?  Well, yes; but whether we *should* is another
question: it might make for many unpleasantly surprising interactions.)
